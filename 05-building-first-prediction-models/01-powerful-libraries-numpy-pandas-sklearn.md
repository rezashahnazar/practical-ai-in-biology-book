[→ فصل ۵: مقدمه](./00-introduction.md) | [بخش ۵-۲: هنر رام کردن داده‌ها: پاک‌سازی و آماده‌سازی ←](./02-art-of-data-wrangling.md)

# فصل ۵: ساخت اولین مدل‌های پیش‌بینی: از داده تا تشخیص

در فصل قبل، شما با ابزارهای پایه‌ای پایتون آشنا شدید و اولین برنامه بیوانفورماتیکی خود را نوشتید. شما اکنون می‌توانید با رشته‌ها کار کنید، حلقه‌ها بنویسید و با دستورات شرطی تصمیم‌گیری کنید. این یک دستاورد بزرگ است. اما برای تحلیل داده‌های عظیم و ساختن مدل‌های هوش مصنوعی، پایتونِ تنها کافی نیست. ما نیاز به ابزارهای تخصصی داریم که برای محاسبات عددی سریع و دستکاری داده‌های پیچیده بهینه شده باشند.

### 🎯 مسئله محوری: چگونه می‌توانیم از یک کدنویس به یک دانشمند داده تبدیل شویم؟

شما پایتون را یاد گرفته‌اید، اما پایتونِ تنها، برای تحلیل داده‌های عظیم ژنومی یا ساخت مدل‌های پیچیده هوش مصنوعی، مانند یک چاقوی سوئیسی در مقابل یک جعبه ابزار مهندسی است. چگونه می‌توانیم محاسبات ریاضی را بر روی میلیون‌ها عدد با سرعت نور انجام دهیم؟ چگونه می‌توانیم داده‌های جدولی عظیم (مانند نتایج توالی‌یابی هزاران بیمار) را به راحتی مدیریت و پاک‌سازی کنیم؟ و مهم‌تر از همه، چگونه می‌توانیم بدون نیاز به اختراع دوباره الگوریتم‌های پیچیده، از مدل‌های آماری و یادگیری ماشین برای پیش‌بینی استفاده کنیم؟ این بخش به ما نشان می‌دهد که چگونه با استفاده از ابزارهای تخصصی، پایتون را به یک پلتفرم تمام‌عیار برای علم داده تبدیل کنیم.

---

## بخش ۵-۱: کتابخانه‌های قدرتمند: آشنایی با NumPy, Pandas و Scikit-learn

یک کتابخانه را مجموعه‌ای از کدها، توابع و ابزارهای از پیش آماده شده در نظر بگیرید که توسط هزاران متخصص در سراسر جهان نوشته و بهینه‌سازی شده‌اند. استفاده از این کتابخانه‌ها به ما اجازه می‌دهد که به جای اختراع دوباره چرخ، بر روی حل مسائل علمی تمرکز کنیم.

### ۱. NumPy: موتور محاسباتی پایتون

**NumPy** (مخفف Numerical Python) بنیادی‌ترین کتابخانه برای محاسبات علمی در پایتون است.

- **هدف اصلی:** فراهم کردن یک ساختار داده قدرتمند به نام **آرایه (Array)** که برای کار با ماتریس‌ها و داده‌های عددی چندبعدی فوق‌العاده سریع و کارآمد است.
- **چرا به آن نیاز داریم؟** عملیات ریاضی بر روی آرایه‌های NumPy (که هسته آن به زبان C نوشته شده) ده‌ها یا حتی صدها برابر سریع‌تر از انجام همان عملیات با لیست‌های عادی پایتون است.
- **آنالوژی:** اگر پایتون استاندارد یک ماشین‌حساب معمولی باشد، NumPy یک ابرکامپیوتر محاسباتی است.

برای استفاده از NumPy، ما ابتدا آن را با یک دستور `import` وارد برنامه خود می‌کنیم. مرسوم است که نام مستعار `np` را برای آن در نظر بگیریم.

```python
import numpy as np

# ساخت یک آرایه NumPy از داده‌های بیان ژن
gene_expressions = np.array([12.5, 4.2, 18.0, 9.7])

print("آرایه بیان ژن:", gene_expressions)

# انجام یک عملیات ریاضی بر روی تمام عناصر به صورت همزمان
normalized_expressions = gene_expressions / 2
print("بیان ژن نرمال‌شده:", normalized_expressions)
```

### ۲. Pandas: جعبه ابزار تحلیل داده

**Pandas** کتابخانه‌ای است که بر روی NumPy ساخته شده و ابزارهای بسیار قدرتمندی برای کار با **داده‌های ساختاریافته و جدولی (Tabular Data)** فراهم می‌کند.

- **هدف اصلی:** معرفی یک ساختار داده به نام **دیتافریم (DataFrame)** که می‌توان آن را مانند یک جدول یا صفحه گسترده (Spreadsheet) هوشمند در نظر گرفت.
- **چرا به آن نیاز داریم؟** تقریباً تمام داده‌های زیستی که با آنها کار خواهیم کرد (مانند لیست بیماران، نتایج آزمایش‌ها، جدول بیان ژن) به صورت جدولی هستند. Pandas خواندن، پاک‌سازی، فیلتر کردن، گروه‌بندی و تحلیل این نوع داده‌ها را فوق‌العاده آسان می‌کند.
- **آنالوژی:** Pandas دفترچه آزمایشگاهی دیجیتال شماست که داده‌ها را به شکلی منظم و قابل تحلیل در آن ثبت می‌کنید.

مرسوم است که Pandas را با نام مستعار `pd` وارد کنیم.

```python
import pandas as pd

# ساخت یک دیتافریم برای ذخیره اطلاعات بیماران
data = {
    'PatientID': ['P001', 'P002', 'P003'],
    'Age': [45, 62, 38],
    'Diagnosis': ['Cancer', 'Healthy', 'Cancer']
}

patient_df = pd.DataFrame(data)

# نمایش کل دیتافریم
print(patient_df)
```

### ۳. Scikit-learn: جعبه ابزار یادگیری ماشین

**Scikit-learn** محبوب‌ترین و جامع‌ترین کتابخانه برای **یادگیری ماشین کلاسیک (Classic Machine Learning)** در پایتون است.

- **هدف اصلی:** فراهم کردن مجموعه‌ای وسیع از الگوریتم‌های آماده برای **طبقه‌بندی (Classification)**، **رگرسیون (Regression)**، **خوشه‌بندی (Clustering)**، **کاهش ابعاد (Dimensionality Reduction)** و **ارزیابی مدل (Model Evaluation)**.
- **چرا به آن نیاز داریم؟** این کتابخانه به ما اجازه می‌دهد تا بدون نیاز به پیاده‌سازی الگوریتم‌های پیچیده از صفر، به سادگی مدل‌های پیش‌بینی‌کننده بسازیم و آنها را آموزش دهیم.
- **آنالوژی:** Scikit-learn جعبه ابزار یک مهندس یادگیری ماشین است که تمام ماشین‌آلات و ابزارهای اندازه‌گیری لازم در آن قرار دارد.

یکی از بهترین ویژگی‌های Scikit-learn، **رابط کاربری یکپارچه (Unified API)** آن است. تمام الگوریتم‌ها از یک الگوی مشابه پیروی می‌کنند:

1.  یک مدل را انتخاب و نمونه‌سازی می‌کنید.
2.  مدل را با داده‌های خود با استفاده از متد `.fit()` **آموزش** می‌دهید.
3.  از مدل آموزش‌دیده برای **پیش‌بینی** بر روی داده‌های جدید با استفاده از متد `.predict()` استفاده می‌کنید.

در بخش‌های بعدی این فصل، ما به صورت عملی از این الگوی `.fit()` و `.predict()` برای ساخت اولین مدل خود استفاده خواهیم کرد.

---

### 🔬 تمرین تحلیلی: انتخاب ابزار مناسب

برای هر یک از سناریوهای زیر، مشخص کنید کدام یک از سه کتابخانه (NumPy, Pandas, Scikit-learn) ابزار اصلی و مناسب‌تر برای انجام آن وظیفه است.

| سناریو                                                                                                         | کتابخانه مناسب (NumPy/Pandas/Scikit-learn) |
| -------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |
| ۱. خواندن یک فایل CSV با ۱۰۰ هزار ردیف از داده‌های بیان ژن و فیلتر کردن بیماران بالای ۵۰ سال.                  |                                            |
| ۲. ساخت یک مدل هوش مصنوعی برای پیش‌بینی اینکه آیا یک تومور بدخیم است یا خوش‌خیم، بر اساس داده‌های تصویربرداری. |                                            |
| ۳. انجام یک عملیات ماتریسی پیچیده بر روی داده‌های سه‌بعدی مغز از یک اسکن fMRI.                                 |                                            |
| ۴. محاسبه میانگین و انحراف معیار ستون "غلظت دارو" در یک جدول بزرگ از نتایج آزمایشگاهی.                         |                                            |

### 💡 نکات کلیدی این بخش

- **کتابخانه‌ها (Libraries)** مجموعه‌ای از کدهای آماده هستند که قابلیت‌های جدیدی به پایتون اضافه می‌کنند.
- **NumPy** برای محاسبات عددی سریع و کار با آرایه‌های چندبعدی (ماتریس‌ها) ضروری است.
- **Pandas** ابزار استاندارد برای کار با داده‌های جدولی (DataFrame) است و برای پاک‌سازی، فیلتر و تحلیل داده‌ها استفاده می‌شود.
- **Scikit-learn** جعبه ابزار اصلی برای ساخت، آموزش و ارزیابی مدل‌های یادگیری ماشین کلاسیک است.
- این سه کتابخانه با هم، ستون فقرات اکوسیستم علم داده در پایتون را تشکیل می‌دهند.

با این سه کتابخانه، ما اکنون برای ورود به دنیای واقعی علم داده و ساخت مدل‌های پیش‌بینی در زیست‌شناسی آماده‌ایم. در بخش بعد، اولین قدم عملی را با آماده‌سازی داده‌ها برای مدل‌سازی برمی‌داریم.

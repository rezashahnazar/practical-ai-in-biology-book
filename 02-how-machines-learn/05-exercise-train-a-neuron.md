[← بخش ۲-۴: مطالعه موردی: AlphaFold چگونه معمای ساختار پروتئین را حل کرد؟](./04-case-study-alphafold.md) | [آزمون فصل دوم →](./exam/index.md)

# فصل ۲: ماشین چگونه یاد می‌گیرد؟

## بخش ۲-۵: بازی عملی: آموزش دستی یک شبکه عصبی

در بخش‌های گذشته با کلمات "یادگیری"، "آموزش" و "تنظیم وزن‌ها" زیاد مواجه شدیم. اما این فرآیند در عمل چگونه است؟ آیا می‌توانیم پرده را کنار بزنیم و خودمان در نقش کامپیوتر ظاهر شویم؟ در این بخش، با یک بازی ساده و با استفاده از قلم و کاغذ، یک "مغز" تک‌نرونی می‌سازیم و به آن یاد می‌دهیم دو نوع سلول زیستی را از هم تشخیص دهد.

### 🎯 مسئله محوری این بخش:

شما یک زیست‌شناس سلولی هستید و دو نوع سلول جدید را زیر میکروسکوپ کشف کرده‌اید: **نوع آلفا (α)** و **نوع بتا (β)**. شما متوجه شده‌اید که این دو نوع سلول بر اساس دو ویژگی اصلی (مثلاً اندازه و زبری غشاء) از هم قابل تمایز هستند. هدف شما ساختن یک "مغز" تک-نرونی است که بتواند با گرفتن این دو ویژگی، نوع سلول را به درستی تشخیص دهد. چگونه می‌توان یک قانون ساده ریاضی طراحی کرد که این دو گروه را از هم جدا کند؟

---

### **مدل ما: یک نرون مصنوعی**

نرون ما یک مدل بسیار ساده است:

- دو ورودی دارد: `x1` (اندازه) و `x2` (زبری).
- هر ورودی یک **وزن (Weight)** دارد: `w1` و `w2`. این وزن‌ها نشان‌دهنده اهمیت هر ویژگی هستند.
- نرون مجموع وزن‌دار ورودی‌ها را محاسبه می‌کند: `Sum = (x1 * w1) + (x2 * w2)`.
- یک **قانون تصمیم‌گیری (آستانه فعال‌سازی)** دارد:
  - اگر `Sum >= 1.0`، نرون فعال می‌شود و خروجی آن **"نوع بتا (β)"** است.
  - اگر `Sum < 1.0`، نرون غیرفعال باقی می‌ماند و خروجی آن **"نوع آلفا (α)"** است.

### **داده‌های آموزشی ما (معلم مجازی)**

در جدول زیر، ۴ نمونه سلول را داریم که نوع واقعی آن‌ها را از قبل می‌دانیم. این "پاسخنامه" ماست.

| نمونه | اندازه (x1) | زبری (x2) | نوع واقعی    |
| :---- | :---------- | :-------- | :----------- |
| **۱** | 0.8         | 0.9       | **β (بتا)**  |
| **۲** | 0.2         | 0.4       | **α (آلفا)** |
| **۳** | 0.9         | 0.3       | **α (آلفا)** |
| **۴** | 0.5         | 0.7       | **β (بتا)**  |

---

### 🔬 تمرین تحلیلی: فرآیند آموزش دستی

ما با وزن‌های کاملاً تصادفی شروع می‌کنیم. فرض کنیم:

- `w1 = 0.5`
- `w2 = 0.5`

**گام اول: پیش‌بینی برای نمونه ۱**

- ورودی: `x1 = 0.8`, `x2 = 0.9`
- محاسبه: `Sum = (0.8 * 0.5) + (0.9 * 0.5) = 0.4 + 0.45 = 0.85`
- تصمیم: `0.85 < 1.0`، پس پیش‌بینی نرون **"نوع آلفا (α)"** است.
- **نتیجه:** اشتباه! نوع واقعی **بتا (β)** بود. خروجی ما خیلی **پایین** بود.
- **اصلاح وزن‌ها:** برای اینکه خروجی بزرگتر شود، باید وزن‌ها را کمی **افزایش** دهیم. بیایید 0.2 به هر دو اضافه کنیم.
  - `w1` جدید = `0.5 + 0.2 = 0.7`
  - `w2` جدید = `0.5 + 0.2 = 0.7`

**گام دوم: پیش‌بینی برای نمونه ۲ (با وزن‌های جدید)**

- وزن‌های فعلی: `w1 = 0.7`, `w2 = 0.7`
- ورودی: `x1 = 0.2`, `x2 = 0.4`
- محاسبه: `Sum = (0.2 * 0.7) + (0.4 * 0.7) = 0.14 + 0.28 = 0.42`
- تصمیم: `0.42 < 1.0`، پس پیش‌بینی نرون **"نوع آلفا (α)"** است.
- **نتیجه:** صحیح! نیازی به تغییر وزن‌ها نیست.

**گام سوم: پیش‌بینی برای نمونه ۳ (با وزن‌های جدید)**

- وزن‌های فعلی: `w1 = 0.7`, `w2 = 0.7`
- ورودی: `x1 = 0.9`, `x2 = 0.3`
- محاسبه: `Sum = (0.9 * 0.7) + (0.3 * 0.7) = 0.63 + 0.21 = 0.84`
- تصمیم: `0.84 < 1.0`، پس پیش‌بینی نرون **"نوع آلفا (α)"** است.
- **نتیجه:** صحیح! نیازی به تغییر وزن‌ها نیست.

**گام چهارم: پیش‌بینی برای نمونه ۴ (با وزن‌های جدید)**

- وزن‌های فعلی: `w1 = 0.7`, `w2 = 0.7`
- ورودی: `x1 = 0.5`, `x2 = 0.7`
- محاسبه: `Sum = (0.5 * 0.7) + (0.7 * 0.7) = 0.35 + 0.49 = 0.84`
- تصمیم: `0.84 < 1.0`، پس پیش‌بینی نرون **"نوع آلفا (α)"** است.
- **نتیجه:** اشتباه! نوع واقعی **بتا (β)** بود. خروجی ما دوباره خیلی **پایین** بود.
- **اصلاح وزن‌ها:** دوباره وزن‌ها را کمی **افزایش** می‌دهیم. این بار 0.3 اضافه می‌کنیم.
  - `w1` نهایی = `0.7 + 0.3 = 1.0`
  - `w2` نهایی = `0.7 + 0.3 = 1.0`

### **پایان آموزش: تست مدل نهایی**

حالا بیایید تمام نمونه‌ها را با وزن‌های نهایی (`w1=1.0, w2=1.0`) دوباره امتحان کنیم:

- **نمونه ۱:** `(0.8*1.0) + (0.9*1.0) = 1.7` (>= 1.0) -> **بتا (β)**. صحیح!
- **نمونه ۲:** `(0.2*1.0) + (0.4*1.0) = 0.6` (< 1.0) -> **آلفا (α)**. صحیح!
- **نمونه ۳:** `(0.9*1.0) + (0.3*1.0) = 1.2` (>= 1.0) -> **بتا (β)**. اشتباه! خروجی خیلی **بالا** بود.

**تحلیل:** ما در گام آخر زیاده‌روی کردیم. افزایش وزن‌ها باعث شد که مدل روی نمونه ۴ درست عمل کند اما روی نمونه ۳ به خطا برود. این یک مفهوم کلیدی به نام **بده-بستان (Trade-off)** در یادگیری ماشین است. ما نیاز به وزن‌هایی داریم که برای _همه_ نمونه‌ها به بهترین شکل ممکن عمل کنند. شاید وزن‌های `w1=0.8` و `w2=0.9` بهتر عمل کنند. (شما می‌توانید این را به عنوان یک تمرین اضافی امتحان کنید!)

### **تجسم مرز تصمیم**

تبریک می‌گویم! شما همین الان یک شبکه عصبی را با دست آموزش دادید. کاری که ما انجام دادیم، پیدا کردن یک "خط مرزی" (Decision Boundary) بود که بتواند سلول‌های آلفا و بتا را از هم جدا کند. وزن‌های نهایی ما `(w1, w2)` معادله این خط را تعیین می‌کنند.

```mermaid
xychart-beta
    title "مرز تصمیم‌گیری نرون"
    x-axis "اندازه (Size)" 0 --> 1
    y-axis "زبری (Granularity)" 0 --> 1

    line "w1=0.7, w2=0.7" [[0, 1.42], [1.42, 0]]

    scatter "نوع آلفا (α)" [[0.2, 0.4], [0.9, 0.3]]
    scatter "نوع بتا (β)" [[0.8, 0.9], [0.5, 0.7]]
```

---

### 💡 نکات کلیدی این بخش

- **یادگیری یک فرآیند تکراری است:** آموزش یک مدل، یک فرآیند گام‌به‌گام برای اصلاح پارامترها (وزن‌ها) بر اساس بازخورد (خطا) از داده‌های آموزشی است.
- **وزن‌ها نشان‌دهنده اهمیت هستند:** وزن‌های یک مدل یاد می‌گیرند که کدام ویژگی‌های ورودی برای رسیدن به تصمیم درست، مهم‌تر هستند.
- **بده-بستان (Trade-off):** پیدا کردن بهترین مدل اغلب به معنی پیدا کردن یک تعادل است که برای همه داده‌ها به طور میانگین خوب عمل کند، نه اینکه برای چند نمونه خاص عالی و برای بقیه ضعیف باشد.
- **از شهود تا ریاضیات:** این بازی دستی، شهود پشت الگوریتم‌های بهینه‌سازی مانند "گرادیان کاهشی" را نشان می‌دهد که همین فرآیند را به صورت ریاضی و در مقیاس بسیار بزرگ انجام می‌دهند.

---

این بازی ساده، جوهره اصلی یادگیری ماشین را نشان می‌دهد. حالا تصور کنید که همین فرآیند را برای یک شبکه با میلیون‌ها وزن و میلیون‌ها نمونه داده، با استفاده از الگوریتم‌های بهینه‌سازی ریاضی و قدرت محاسباتی کامپیوترها انجام دهید. این همان جادویی است که در پشت سیستم‌هایی مانند AlphaFold قرار دارد. در بخش نهایی این فصل، تمام این مفاهیم را در یک آزمون جامع مرور خواهیم کرد.

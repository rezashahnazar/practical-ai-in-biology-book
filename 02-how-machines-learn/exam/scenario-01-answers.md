[→ سناریو ۱: سوالات](./scenario-01-questions.md) | [پاسخنامه سناریو ۲ ←](./scenario-02-answers.md)

## سناریوی ۱: پاسخنامه و تحلیل سوالات

### پاسخ سوال ۱

**پاسخ صحیح: گزینه ج (یادگیری با نظارت - طبقه‌بندی)**

**تحلیل:**

- **پارادایم یادگیری:** مسئله ما شامل یک مجموعه داده است که در آن هر نمونه (پروتئین) دارای یک سری ویژگی (features) و یک برچسب (label) از پیش تعیین‌شده و صحیح (جایگاه سلولی) است. هدف، یادگیری یک مدل است که بتواند ارتباط بین ویژگی‌ها و برچسب را یاد بگیرد تا برای داده‌های جدید، برچسب صحیح را پیش‌بینی کند. این تعریف دقیق **یادگیری با نظارت (Supervised Learning)** است. در یادگیری بدون نظارت، ما برچسب‌های از پیش تعیین‌شده نداریم و هدف، کشف الگوهای ذاتی در داده‌هاست. یادگیری تقویتی نیز مربوط به یادگیری از طریق آزمون و خطا در یک محیط تعاملی است که در اینجا کاربرد ندارد.
- **نوع وظیفه:** متغیر هدف ما، یعنی "کلاس (جایگاه)"، یک متغیر **دسته‌ای (Categorical)** و گسسته است (CYT, MIT, NUC, ...). وظیفه‌ای که در آن یک مدل یاد می‌گیرد تا یک نمونه را به یکی از چندین کلاس از پیش تعریف‌شده اختصاص دهد، **طبقه‌بندی (Classification)** نامیده می‌شود. اگر هدف ما پیش‌بینی یک مقدار پیوسته (مانند سطح بیان یک پروتئین) بود، آنگاه وظیفه از نوع رگرسیون (Regression) می‌بود.

بنابراین، ترکیب صحیح، «یادگیری با نظارت» و «طبقه‌بندی» است.

### پاسخ سوال ۲

**پاسخ صحیح: گزینه الف (صحیح)**

**تحلیل:**

این گزاره کاملاً صحیح است. شبکه‌های عصبی، به‌ویژه آن‌هایی که با الگوریتم‌های مبتنی بر گرادیان (مانند Gradient Descent) آموزش داده می‌شوند، به مقیاس ویژگی‌های ورودی حساس هستند. دلایل اصلی این امر عبارتند از:

1.  **همگرایی سریع‌تر:** اگر ویژگی‌ها مقیاس‌های بسیار متفاوتی داشته باشند (مثلاً یک ویژگی در محدوده ۰ تا ۱ و دیگری در محدوده ۱۰۰۰ تا ۱۰۰۰۰)، سطح خطای مدل به یک بیضی کشیده تبدیل می‌شود. الگوریتم بهینه‌سازی برای یافتن نقطه بهینه در این سطح ناهموار، مسیر پر پیچ و خمی را طی می‌کند و به کندی همگرا می‌شود. با مقیاس‌بندی ویژگی‌ها، سطح خطا به شکل دایره‌مانندتری درمی‌آید و الگوریتم بهینه‌سازی می‌تواند با سرعت و پایداری بیشتری به سمت نقطه بهینه حرکت کند.
2.  **جلوگیری از غلبه ویژگی‌های با مقیاس بزرگ:** وزن‌های مرتبط با ویژگی‌هایی که مقادیر عددی بزرگتری دارند، در ابتدای آموزش باید مقادیر کوچکتری بگیرند تا اثر آن‌ها کنترل شود. این مسئله می‌تواند باعث ناپایداری در فرآیند یادگیری شود. استانداردسازی (Standardization) یا نرمال‌سازی (Normalization) تضمین می‌کند که همه ویژگی‌ها از ابتدا سهم یکسانی در محاسبات اولیه دارند.

بنابراین، مقیاس‌بندی ویژگی‌ها یک گام پیش‌پردازش استاندارد و ضروری برای اکثر مدل‌های شبکه عصبی است.

### پاسخ سوال ۳

**پاسخ صحیح: گزینه ج (ورودی: ۸ نورون، خروجی: ۱۰ نورون)**

**تحلیل:**

- **لایه ورودی (Input Layer):** تعداد نورون‌ها در لایه ورودی یک شبکه عصبی باید برابر با تعداد ویژگی‌هایی باشد که برای هر نمونه داده به مدل ارائه می‌شود. در این مسئله، ما ۸ ویژگی بیوشیمیایی (`mcg`, `gvh`, `alm`, `mit`, `erl`, `pox`, `vac`, `nuc`) برای هر پروتئین داریم. ستون "نام پروتئین" یک شناسه است و به عنوان ویژگی ورودی استفاده نمی‌شود. بنابراین، لایه ورودی باید **۸ نورون** داشته باشد.
- **لایه خروجی (Output Layer):** در یک مسئله طبقه‌بندی چندکلاسه، تعداد نورون‌های لایه خروجی باید برابر با تعداد کلاس‌های ممکن باشد. در سناریو ذکر شده که ۱۰ کلاس (جایگاه) مختلف برای پروتئین‌ها وجود دارد. برای اینکه مدل بتواند برای هر نمونه، احتمال تعلق به هر یک از این ۱۰ کلاس را پیش‌بینی کند، لایه خروجی باید **۱۰ نورون** داشته باشد. هر نورون مسئول پیش‌بینی احتمال یک کلاس خاص خواهد بود.

### پاسخ سوال ۴

**پاسخ صحیح: گزینه ج (تابع فعال‌سازی: Softmax، تابع هزینه: Categorical Cross-Entropy)**

**تحلیل:**

این ترکیب، استاندارد طلایی برای مسائل طبقه‌بندی چندکلاسه است.

- **تابع فعال‌سازی لایه خروجی (Softmax):** تابع Softmax خروجی‌های خام (logits) لایه آخر شبکه را می‌گیرد و آن‌ها را به یک توزیع احتمال تبدیل می‌کند. یعنی مقادیر خروجی ۱۰ نورون را به گونه‌ای تبدیل می‌کند که همگی بین ۰ و ۱ بوده و مجموع آن‌ها برابر با ۱ باشد. این دقیقاً همان چیزی است که ما نیاز داریم: یک بردار از احتمالات که نشان می‌دهد نمونه ورودی با چه احتمالی به هر یک از ۱۰ کلاس تعلق دارد. کلاسی که بالاترین احتمال را دارد، به عنوان پیش‌بینی نهایی مدل انتخاب می‌شود.
- **تابع هزینه (Categorical Cross-Entropy):** این تابع هزینه برای اندازه‌گیری فاصله بین دو توزیع احتمال طراحی شده است: توزیع احتمال پیش‌بینی‌شده توسط مدل (خروجی Softmax) و توزیع احتمال واقعی (که یک بردار one-hot است، یعنی برای کلاس صحیح مقدار ۱ و برای بقیه ۰ است). هدف مدل در طول آموزش، به حداقل رساندن این فاصله یا "آنتروپی متقاطع" است که معادل با به حداکثر رساندن احتمال کلاس صحیح است.

**چرا گزینه‌های دیگر نامناسب هستند؟**

- **الف:** Sigmoid و Mean Squared Error (MSE) معمولاً برای مسائل رگرسیون یا طبقه‌بندی باینری (با یک نورون خروجی) استفاده می‌شوند و برای طبقه‌بندی چندکلاسه مناسب نیستند.
- **ب:** ReLU یک تابع فعال‌سازی عالی برای لایه‌های پنهان است اما برای لایه خروجی طبقه‌بندی مناسب نیست زیرا خروجی آن محدود به مقادیر مثبت است و توزیع احتمال تولید نمی‌کند. MAE نیز یک تابع هزینه برای مسائل رگرسیون است.
- **د:** Tanh نیز یک تابع فعال‌سازی برای لایه‌های پنهان است. Binary Cross-Entropy به طور خاص برای مسائل طبقه‌بندی باینری (دوکلاسه) طراحی شده است و برای مسئله ما که ۱۰ کلاس دارد، مناسب نیست.

[→ سناریو ۲: سوالات](./scenario-02-questions.md) | [سناریو ۳: پاسخنامه ←](./scenario-03-answers.md)

# پاسخنامه و تحلیل سناریو ۲: انتخاب معماری یادگیری عمیق

---

### تحلیل و پاسخ سوال ۱

**پاسخ صحیح:** (الف) معماری A (CNN)، زیرا در شناسایی الگوهای محلی و مستقل از مکان (position-invariant) مانند موتیف‌ها تخصص دارد.

**تحلیل:**

- **ماهیت مسئله:** وظیفه اصلی، شناسایی یک **موتیف** است. موتیف یک الگوی کوتاه و محلی است. مهم‌ترین ویژگی که مدل باید داشته باشد، توانایی تشخیص این الگو، **صرف‌نظر از مکان دقیق آن** در توالی ۲۰۰ جفت‌بازی است.
- **قدرت CNN:** شبکه‌های عصبی کانولوشنی (CNN) دقیقاً برای همین کار طراحی شده‌اند. فیلترهای (یا کرنل‌های) کانولوشنی مانند یک "پنجره لغزان" عمل می‌کنند که در طول توالی حرکت کرده و به دنبال الگوی خاصی (که یاد گرفته‌اند) می‌گردند. وقتی فیلتر، الگوی مورد نظر (موتیف) را پیدا می‌کند، یک سیگنال قوی تولید می‌کند. این فرآیند ذاتاً **مستقل از مکان (position-invariant)** است؛ یعنی برای CNN فرقی نمی‌کند که موتیف در ابتدای توالی باشد یا در انتها.
- **محدودیت RNN:** در مقابل، شبکه‌های عصبی بازگشتی (RNN) برای درک **ترتیب و وابستگی‌های زمانی (یا مکانی در توالی)** طراحی شده‌اند. اگرچه می‌توانند الگوها را تشخیص دهند، اما قدرت اصلی آن‌ها در درک "زمینه" و روابط بین عناصر توالی است. برای مسئله ساده‌ی پیدا کردن یک موتیف، این قابلیت بیش از حد نیاز است و ممکن است به خوبی CNN عمل نکند.
- **نتیجه‌گیری:** برای وظیفه شناسایی یک الگوی محلی و مستقل از مکان، CNN معماری کارآمدتر و مستقیم‌تری است. این معماری به طور گسترده و با موفقیت در زمینه ژنومیک برای شناسایی موتیف‌ها به کار رفته است.

---

### تحلیل و پاسخ سوال ۲

**پاسخ صحیح:** (ب) RNNها در به خاطر سپردن اطلاعات برای توالی‌های بسیار طولانی (مشکل محو شدن گرادیان یا Vanishing Gradient) دچار چالش می‌شوند و ممکن است موتیفی که در ابتدای توالی ظاهر شده را تا انتهای پردازش "فراموش" کنند.

**تحلیل:**

- **مشکل ذاتی RNNها:** شبکه‌های RNN استاندارد، اطلاعات را به صورت یک "حافظه" یا "حالت پنهان" از یک مرحله به مرحله بعد منتقل می‌کنند. در تئوری، این حافظه باید بتواند اطلاعات مهم از ابتدای توالی را تا انتها حفظ کند.
- **محو شدن گرادیان (Vanishing Gradient):** در عمل، هنگام آموزش RNNها با استفاده از الگوریتم پس‌انتشار (Backpropagation)، گرادیان‌ها (سیگنال‌های خطا) با عبور از هر مرحله زمانی، به طور مکرر در ماتریس‌های وزن ضرب می‌شوند. اگر مقادیر این ماتریس‌ها کوچک باشند، گرادیان به سرعت به سمت صفر میل می‌کند. این پدیده باعث می‌شود که شبکه نتواند وزن‌های مراحل اولیه را به درستی تنظیم کند و در عمل، اطلاعاتی که در ابتدای توالی دیده‌ شده را "فراموش" می‌کند.
- **تأثیر در مسئله ما:** برای یک توالی DNA به طول ۲۰۰، اگر موتیف در ۱۰ جفت‌باز اول ظاهر شود، یک RNN استاندارد ممکن است تا زمانی که به انتهای توالی می‌رسد، تأثیر آن را از دست داده باشد. (توجه: معماری‌های پیشرفته‌تری مانند **LSTM** و **GRU** دقیقاً برای حل همین مشکل طراحی شده‌اند، اما سوال در مورد "RNN استاندارد" است).
- **چرا گزینه‌های دیگر غلط هستند؟**
  - **(الف):** RNNها دقیقاً برای پردازش داده‌های توالی‌مانند (sequential data) ساخته شده‌اند.
  - **(ج):** برعکس، CNNها به دلیل قابلیت موازی‌سازی بالا در محاسبات (پردازش بخش‌های مختلف توالی به صورت همزمان)، معمولاً از RNNها (که ذاتاً ترتیبی و سریال هستند) بسیار سریع‌تر آموزش می‌بینند.
  - **(د):** RNNها قادر به یادگیری الگوهای بسیار پیچیده هستند و برای مسائل ساده محدود نشده‌اند. مشکل آن‌ها ساختاری است، نه ناتوانی در یادگیری پیچیدگی.

---

### تحلیل و پاسخ سوال ۳

**پاسخ صحیح:** (ب) این کشف، اهمیت معماری RNN را افزایش می‌دهد، زیرا اکنون درک روابط و فواصل بلندمدت بین دو موتیف برای پیش‌بینی صحیح، حیاتی می‌شود.

**تحلیل:**

- **تغییر ماهیت مسئله:** با این کشف جدید، مسئله از "پیدا کردن یک الگوی منفرد" به "پیدا کردن دو الگو و درک رابطه مکانی بین آن‌ها" تغییر می‌کند. اکنون دیگر فقط وجود موتیف‌ها مهم نیست، بلکه **فاصله و ترتیب** آن‌ها نیز اهمیت پیدا کرده است.
- **قدرت RNN در این سناریو:** این دقیقاً همان جایی است که معماری‌های بازگشتی (RNN) برتری خود را نشان می‌دهند. یک مدل مبتنی بر RNN (به خصوص LSTM یا GRU) می‌تواند پس از شناسایی موتیف اول، این اطلاعات را در حالت پنهان خود حفظ کرده و سپس با شناسایی موتیف دوم، رابطه بین آن‌ها (اینکه در فاصله ۵۰ تا ۱۰۰ جفت‌بازی قرار دارند) را یاد بگیرد.
- **محدودیت CNN در این سناریو:** یک CNN استاندارد در شناسایی هر دو موتیف به صورت جداگانه عالی عمل می‌کند، اما ذاتاً برای درک روابط و فواصل بلندمدت بین ویژگی‌هایی که پیدا کرده، طراحی نشده است. لایه‌های بالاتر در یک CNN می‌توانند ترکیب‌های محلی از ویژگی‌ها را یاد بگیرند، اما مدل‌سازی یک فاصله متغیر (۵۰ تا ۱۰۰ جفت‌باز) بین دو موتیف برای آن چالش‌برانگیز است.
- **نتیجه‌گیری:** کشف جدید، مسئله را به یک وظیفه "درک زمینه و روابط ساختاری" نزدیک‌تر می‌کند که نقطه قوت اصلی RNNها است. در عمل، بهترین معماری برای چنین مسئله‌ای ممکن است یک **مدل ترکیبی (Hybrid Model)** باشد که از لایه‌های CNN برای تشخیص موتیف‌ها و از لایه‌های RNN برای مدل‌سازی روابط بین آن‌ها استفاده می‌کند. اما بین دو گزینه مطرح‌شده، اهمیت RNN به وضوح افزایش می‌یابد.
